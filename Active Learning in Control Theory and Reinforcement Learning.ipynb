{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Learning in Control Theory and Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook was used for the experiments in the thesis 'Active Learning in Control Theory and Reinforcement Learning'. The code is provided as I worked with it to make the experiments reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports that are needed later on\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import eigh\n",
    "from math import sqrt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "import scipy.optimize\n",
    "from scipy.optimize import minimize\n",
    "from control.matlab import *  # MATLAB-like functions\n",
    "import random\n",
    "from datetime import datetime\n",
    "from joblib import delayed, Parallel\n",
    "import gym\n",
    "from gym import wrappers\n",
    "from gym import spaces\n",
    "import json\n",
    "import joblib\n",
    "import shutil\n",
    "import os.path as osp, time, atexit, os\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# For the PPO implementation of OpenAI, see\n",
    "# https://spinningup.openai.com/en/latest/\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import spinup.algos.pytorch.ppo.core as core\n",
    "from spinup.utils.logx import EpochLogger\n",
    "from spinup.utils.mpi_pytorch import setup_pytorch_for_mpi, sync_params, mpi_avg_grads\n",
    "from spinup.utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "from spinup import ppo_pytorch as ppo\n",
    "from spinup.algos.pytorch.ppo.ppo import PPOBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of the control methods and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some serialization and logger methods, taken from https://github.com/openai/spinningup/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/openai/spinningup/blob/master/spinup/utils/serialization_utils.py\n",
    "\n",
    "\n",
    "def convert_json(obj):\n",
    "    \"\"\" Convert obj to a version which can be serialized with JSON. \"\"\"\n",
    "    if is_json_serializable(obj):\n",
    "        return obj\n",
    "    else:\n",
    "        if isinstance(obj, dict):\n",
    "            return {convert_json(k): convert_json(v) \n",
    "                    for k,v in obj.items()}\n",
    "\n",
    "        elif isinstance(obj, tuple):\n",
    "            return (convert_json(x) for x in obj)\n",
    "\n",
    "        elif isinstance(obj, list):\n",
    "            return [convert_json(x) for x in obj]\n",
    "\n",
    "        elif hasattr(obj,'__name__') and not('lambda' in obj.__name__):\n",
    "            return convert_json(obj.__name__)\n",
    "\n",
    "        elif hasattr(obj,'__dict__') and obj.__dict__:\n",
    "            obj_dict = {convert_json(k): convert_json(v) \n",
    "                        for k,v in obj.__dict__.items()}\n",
    "            return {str(obj): obj_dict}\n",
    "\n",
    "        return str(obj)\n",
    "    \n",
    "def is_json_serializable(v):\n",
    "    try:\n",
    "        json.dumps(v)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/openai/spinningup/blob/master/spinup/utils/logx.py\n",
    "\n",
    "\n",
    "color2num = dict(\n",
    "    gray=30,\n",
    "    red=31,\n",
    "    green=32,\n",
    "    yellow=33,\n",
    "    blue=34,\n",
    "    magenta=35,\n",
    "    cyan=36,\n",
    "    white=37,\n",
    "    crimson=38\n",
    ")\n",
    "\n",
    "def colorize(string, color, bold=False, highlight=False):\n",
    "    \"\"\"\n",
    "    Colorize a string.\n",
    "    This function was originally written by John Schulman.\n",
    "    \"\"\"\n",
    "    attr = []\n",
    "    num = color2num[color]\n",
    "    if highlight: num += 10\n",
    "    attr.append(str(num))\n",
    "    if bold: attr.append('1')\n",
    "    return '\\x1b[%sm%s\\x1b[0m' % (';'.join(attr), string)\n",
    "class Logger:\n",
    "    \"\"\"\n",
    "    A general-purpose logger.\n",
    "    Makes it easy to save diagnostics, hyperparameter configurations, the \n",
    "    state of a training run, and the trained model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_dir=None, output_fname='progress.txt', exp_name=None):\n",
    "        \"\"\"\n",
    "        Initialize a Logger.\n",
    "        Args:\n",
    "            output_dir (string): A directory for saving results to. If \n",
    "                ``None``, defaults to a temp directory of the form\n",
    "                ``/tmp/experiments/somerandomnumber``.\n",
    "            output_fname (string): Name for the tab-separated-value file \n",
    "                containing metrics logged throughout a training run. \n",
    "                Defaults to ``progress.txt``. \n",
    "            exp_name (string): Experiment name. If you run multiple training\n",
    "                runs and give them all the same ``exp_name``, the plotter\n",
    "                will know to group them. (Use case: if you run the same\n",
    "                hyperparameter configuration with multiple random seeds, you\n",
    "                should give them all the same ``exp_name``.)\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir or \"/tmp/experiments/%i\"%int(time.time())\n",
    "        if osp.exists(self.output_dir):\n",
    "            print(\"Warning: Log dir %s already exists! Storing info there anyway.\"%self.output_dir)\n",
    "        else:\n",
    "            os.makedirs(self.output_dir)\n",
    "        self.output_file = open(osp.join(self.output_dir, output_fname), 'w')\n",
    "        atexit.register(self.output_file.close)\n",
    "        print(colorize(\"Logging data to %s\"%self.output_file.name, 'green', bold=True))\n",
    "        self.first_row=True\n",
    "        self.log_headers = []\n",
    "        self.log_current_row = {}\n",
    "        self.exp_name = exp_name\n",
    "\n",
    "    def log(self, msg, color='green'):\n",
    "        \"\"\"Print a colorized message to stdout.\"\"\"\n",
    "        print(colorize(msg, color, bold=True))\n",
    "\n",
    "    def log_tabular(self, key, val):\n",
    "        \"\"\"\n",
    "        Log a value of some diagnostic.\n",
    "        Call this only once for each diagnostic quantity, each iteration.\n",
    "        After using ``log_tabular`` to store values for each diagnostic,\n",
    "        make sure to call ``dump_tabular`` to write them out to file and\n",
    "        stdout (otherwise they will not get saved anywhere).\n",
    "        \"\"\"\n",
    "        if self.first_row:\n",
    "            self.log_headers.append(key)\n",
    "        else:\n",
    "            assert key in self.log_headers, \"Trying to introduce a new key %s that you didn't include in the first iteration\"%key\n",
    "        assert key not in self.log_current_row, \"You already set %s this iteration. Maybe you forgot to call dump_tabular()\"%key\n",
    "        self.log_current_row[key] = val\n",
    "\n",
    "    def save_config(self, config):\n",
    "        \"\"\"\n",
    "        Log an experiment configuration.\n",
    "        Call this once at the top of your experiment, passing in all important\n",
    "        config vars as a dict. This will serialize the config to JSON, while\n",
    "        handling anything which can't be serialized in a graceful way (writing\n",
    "        as informative a string as possible). \n",
    "        Example use:\n",
    "        .. code-block:: python\n",
    "            logger = EpochLogger(**logger_kwargs)\n",
    "            logger.save_config(locals())\n",
    "        \"\"\"\n",
    "        config_json = convert_json(config)\n",
    "        if self.exp_name is not None:\n",
    "            config_json['exp_name'] = self.exp_name\n",
    "        output = json.dumps(config_json, separators=(',',':\\t'), indent=4, sort_keys=True)\n",
    "        print(colorize('Saving config:\\n', color='cyan', bold=True))\n",
    "        #print(output)\n",
    "        with open(osp.join(self.output_dir, \"config.json\"), 'w') as out:\n",
    "            out.write(output)\n",
    "\n",
    "    def save_state(self, state_dict, itr=None):\n",
    "        \"\"\"\n",
    "        Saves the state of an experiment.\n",
    "        To be clear: this is about saving *state*, not logging diagnostics.\n",
    "        All diagnostic logging is separate from this function. This function\n",
    "        will save whatever is in ``state_dict``---usually just a copy of the\n",
    "        environment---and the most recent parameters for the model you \n",
    "        previously set up saving for with ``setup_tf_saver``. \n",
    "        Call with any frequency you prefer. If you only want to maintain a\n",
    "        single state and overwrite it at each call with the most recent \n",
    "        version, leave ``itr=None``. If you want to keep all of the states you\n",
    "        save, provide unique (increasing) values for 'itr'.\n",
    "        Args:\n",
    "            state_dict (dict): Dictionary containing essential elements to\n",
    "                describe the current state of training.\n",
    "            itr: An int, or None. Current iteration of training.\n",
    "        \"\"\"\n",
    "        fname = 'vars.pkl' if itr is None else 'vars%d.pkl'%itr\n",
    "        try:\n",
    "            joblib.dump(state_dict, osp.join(self.output_dir, fname))\n",
    "        except:\n",
    "            self.log('Warning: could not pickle state_dict.', color='red')\n",
    "        if hasattr(self, 'tf_saver_elements'):\n",
    "            self._tf_simple_save(itr)\n",
    "        if hasattr(self, 'pytorch_saver_elements'):\n",
    "            self._pytorch_simple_save(itr)\n",
    "\n",
    "\n",
    "    def dump_tabular(self):\n",
    "        \"\"\"\n",
    "        Write all of the diagnostics from the current iteration.\n",
    "        Writes both to stdout, and to the output file.\n",
    "        \"\"\"\n",
    "        vals = []\n",
    "        key_lens = [len(key) for key in self.log_headers]\n",
    "        max_key_len = max(15,max(key_lens))\n",
    "        keystr = '%'+'%d'%max_key_len\n",
    "        fmt = \"| \" + keystr + \"s | %15s |\"\n",
    "        n_slashes = 22 + max_key_len\n",
    "        #print(\"-\"*n_slashes)\n",
    "        for key in self.log_headers:\n",
    "            val = self.log_current_row.get(key, \"\")\n",
    "            valstr = \"%8.3g\"%val if hasattr(val, \"__float__\") else val\n",
    "            #print(fmt%(key, valstr))\n",
    "            vals.append(val)\n",
    "        #print(\"-\"*n_slashes, flush=True)\n",
    "        if self.output_file is not None:\n",
    "            if self.first_row:\n",
    "                self.output_file.write(\"\\t\".join(self.log_headers)+\"\\n\")\n",
    "            self.output_file.write(\"\\t\".join(map(str,vals))+\"\\n\")\n",
    "            self.output_file.flush()\n",
    "        self.log_current_row.clear()\n",
    "        self.first_row=False\n",
    "        \n",
    "    def close(self):\n",
    "        self.output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runningMean(x, N):\n",
    "    return np.convolve(x, np.ones(N)/N, mode='same')\n",
    "\n",
    "def deaggregate(y):\n",
    "    x = np.copy(y.values)\n",
    "    for i in range(len(x)-1):\n",
    "        x[len(x)-i-1] -= x[len(x)-i-2]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global vars for tracking and labeling data at load time.\n",
    "exp_idx = 0\n",
    "units = dict()\n",
    "# False if you want to compare different random seeds in a single plot\n",
    "#condition = True\n",
    "\n",
    "def get_datasets(logdir, condition=None, exclude=False, trajectoryAmount=201, trajectoryLength=200, runningMeanSize=1):\n",
    "    \"\"\"\n",
    "    Recursively look through logdir for output files produced by\n",
    "    spinup.logx.Logger. \n",
    "    Assumes that any file \"progress.txt\" is a valid hit. \n",
    "    \"\"\"\n",
    "    global exp_idx\n",
    "    global units\n",
    "    datasets = []\n",
    "    for root, _, files in os.walk(logdir):\n",
    "        if exclude and 'n=' in root:\n",
    "            continue\n",
    "        if 'progress.txt' in files:\n",
    "            exp_name = None\n",
    "            try:\n",
    "                config_path = open(os.path.join(root,'config.json'))\n",
    "                config = json.load(config_path)\n",
    "                if 'exp_name' in config:\n",
    "                    exp_name = config['exp_name']\n",
    "            except:\n",
    "                print('No file named config.json')\n",
    "            condition1 = condition or exp_name or 'exp'\n",
    "            condition2 = condition1 + '-' + str(exp_idx)\n",
    "            exp_idx += 1\n",
    "            if condition1 not in units:\n",
    "                units[condition1] = 0\n",
    "            unit = units[condition1]\n",
    "            units[condition1] += 1\n",
    "\n",
    "            try:\n",
    "                exp_data = pd.read_table(os.path.join(root,'progress.txt'))\n",
    "                if (len(exp_data) < trajectoryAmount):\n",
    "                    print('Did not contain every trajectory: %s'%os.path.join(root,'progress.txt'))\n",
    "                    continue\n",
    "            except:\n",
    "                print('Could not read from %s'%os.path.join(root,'progress.txt'))\n",
    "                continue\n",
    "            performance = 'MSE'\n",
    "            exp_data.insert(len(exp_data.columns),'Unit',unit)\n",
    "            exp_data.insert(len(exp_data.columns),'Learner',condition1)\n",
    "            exp_data.insert(len(exp_data.columns),'Condition2',condition2)\n",
    "            exp_data.insert(len(exp_data.columns),'Performance',exp_data[performance])\n",
    "            exp_data.insert(len(exp_data.columns),'Running reward',runningMean(exp_data[\"Reward\"].to_numpy(), runningMeanSize))\n",
    "            exp_data.insert(len(exp_data.columns),'Cost of trajectory', runningMean(deaggregate(exp_data[\"Cost\"]), runningMeanSize))\n",
    "            datasets.append(exp_data)\n",
    "            config_path.close()\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_palette = dict(zip([\"PPO\", \"Random\", \"Optimal\", \"Passive\", \"DC max\", \"DC 1/n\"],sns.color_palette(\"tab10\")))\n",
    "\n",
    "### Generates the cost, mean squared error, reward and efficiency plots\n",
    "### from the data in the given directory and saves them there.\n",
    "### If useEstimation, the additional plot of the estimation error\n",
    "### is produced.\n",
    "def generatePlots(dataDirectory, trajectoryAmount=201, useEstimation=False, runningMeanSize=1):\n",
    "    data = (get_datasets(dataDirectory, exclude=False, trajectoryAmount=trajectoryAmount, runningMeanSize=runningMeanSize))\n",
    "    if isinstance(data, list):\n",
    "        data = pd.concat(data, ignore_index=True)\n",
    "    # Change old names\n",
    "    data.loc[data[\"Learner\"] == \"Optimal control\", \"Learner\"] = \"Optimal\"\n",
    "    data.loc[data[\"Learner\"] == \"Passive Learner\", \"Learner\"] = \"Passive\"\n",
    "    data.loc[data[\"Learner\"] == \"Random Control\", \"Learner\"] = \"Random\"\n",
    "    data.loc[data[\"Learner\"] == \"WDC 1/n\", \"Learner\"] = \"DC 1/n\"\n",
    "    data.loc[(data[\"Learner\"] == \"WDC MAX\") | (data[\"Learner\"] == \"DC MAX\"), \"Learner\"] = \"DC max\"\n",
    "    data.loc[data[\"Learner\"] == \"PPO all\", \"Learner\"] = \"PPO\"\n",
    "    try:\n",
    "        data.rename(columns = {'Decrease in MSE per Cost': 'Decrease in MSE per cost', \n",
    "                               \"Running Reward\": \"Running reward\"}, inplace=True)\n",
    "    except:\n",
    "        pass\n",
    "    sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "    values = [\"Cost\", \"Cost of trajectory\", \"Trajectory index\", \"MSE\", \"Decrease in MSE per cost\", \"Running reward\"]\n",
    "    if useEstimation:\n",
    "        values += [ \"Estimation\" ]\n",
    "    for value in values:\n",
    "        plt.figure(figsize=(10, 7.5))\n",
    "        plt.tight_layout()\n",
    "        data2 = data.copy(deep=True)\n",
    "        if (value in [\"MSE\", \"Decrease in MSE per cost\", \"Estimation\"]):\n",
    "            data2 = data2[data2[\"Learner\"] != \"PPO\"]\n",
    "        if (value in [\"Estimation\", \"MSE\", \"Decrease in MSE per cost\"]):\n",
    "            data2 = data2[data2[\"Learner\"] != \"Optimal\"]\n",
    "        g = sns.lineplot(data=data2,#[data[\"Learner\"]!=\"Random Control\"],\n",
    "                         x=\"Trajectory number\", \n",
    "                         y=value, \n",
    "                         hue=\"Learner\", \n",
    "                         ci='sd', \n",
    "                         palette=color_palette,\n",
    "                         linewidth=3,\n",
    "                         estimator=getattr(np, 'median'))\n",
    "        if (value == \"Trajectory index\"):\n",
    "            g.set(ylabel=\"Trajectory steps\")\n",
    "        if (value == \"Running reward\" or value == \"Cost of trajectory\"):\n",
    "            g.set_xlim(runningMeanSize, trajectoryAmount-runningMeanSize)\n",
    "            g.set_ylim(auto=True)\n",
    "            if (value ==\"Cost of trajectory\"):\n",
    "                g.set(yscale=\"log\")\n",
    "        else:\n",
    "            if (value != \"Trajectory index\"):\n",
    "                g.set(yscale=\"log\")\n",
    "        plt.legend(loc='best').set_draggable(True)\n",
    "        plt.savefig(dataDirectory + \"/%s\"%value, dpi=300, bbox_inches='tight')\n",
    "    # Trajectory reached plot\n",
    "    counts = data.groupby([\"Learner\", \"Unit\"]).mean().loc[:, \"Trajectory index\"].unstack(0)\n",
    "    sns.set(style=\"darkgrid\", font_scale=1.5)\n",
    "    plt.figure(figsize=(10, 7.5))\n",
    "    plt.tight_layout()\n",
    "    order = [\"PPO\", \"Optimal\", \"DC max\", \"DC 1/n\", \"Random\", \"Passive\"] if useEstimation else [\"PPO\", \"DC max\", \"DC 1/n\", \"Random\", \"Passive\"]\n",
    "    g = sns.barplot(data=counts, \n",
    "                palette=color_palette, \n",
    "                order=order,\n",
    "               estimator=getattr(np, 'median'),\n",
    "                   ci='sd')\n",
    "    g.set(ylabel=\"Trajectory steps\")\n",
    "    plt.savefig(dataDirectory + \"/Trajectory reached\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the least-squares estimator which will be used in identifying the system matrices in a linear dynamical system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Least-squares estimator for given data X and Y. If there are not sufficient samples to calculate the least-squares estimator,\n",
    "### an exception is raised. Otherwise, the dispersion matrix is (inefficiently computed) stored in the dispersion variable.\n",
    "class LSE():\n",
    "    def __init__(self, trainingData=np.array([]), trainingDataLabel=np.array([])):\n",
    "        self.X = trainingData\n",
    "        self.Y = trainingDataLabel\n",
    "        self.XTX = trainingData.T @ trainingData\n",
    "        if (self.isXTXFullRank()):\n",
    "            self.dispersion = np.linalg.inv(self.XTX)\n",
    "            # The matrix is symmetric, but rounding errors render it\n",
    "            # slightly non-symmetric sometimes.\n",
    "            for i in range(self.dispersion.shape[0]):\n",
    "                for j in range(i+1, self.dispersion.shape[1]):\n",
    "                    self.dispersion[i, j] = self.dispersion[j, i]\n",
    "        else:\n",
    "            raise Exception(\"The training data did not have enough examples!\")\n",
    "        \n",
    "        \n",
    "    def isXTXFullRank(self):\n",
    "        if (self.XTX.shape == ()):\n",
    "            return False\n",
    "        return np.linalg.matrix_rank(self.XTX) == self.XTX.shape[0]\n",
    "    \n",
    "    def MSE(self):\n",
    "        return np.trace(self.dispersion)\n",
    "    \n",
    "    # [A^T; B^T]\n",
    "    def parameterEstimation(self):\n",
    "        return self.dispersion @ self.X.T @ self.Y\n",
    "    \n",
    "    def addDataRow(self, x, y):\n",
    "        self.X = np.vstack([self.X, x.T])\n",
    "        self.Y = np.vstack([self.Y, y.T])\n",
    "        self.XTX = self.XTX + x @ x.T\n",
    "        self.dispersion = self.dispersion - (self.dispersion @ x @ x.T @ self.dispersion) / (1 + x.T @ self.dispersion @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### A common base class for the evaluated control methods.\n",
    "class ControlMethod():\n",
    "    # To get an estimate for the system parameters, initial data is neccessary.\n",
    "# It is the same across a single test run to compare the control methods.\n",
    "# trajectoryNumber is the number of the current trajectory, i.e. if there are 10 total trajectories planned\n",
    "# and the experiment is in the third, this will be 3 - 1\n",
    "# trajectoryIndex: Index in the current trajectory. Will not be greater than trajectoryLength.\n",
    "    def __init__(self,\n",
    "                 exp_name, \n",
    "                 trainingData=np.array([]), \n",
    "                 trainingDataLabel=np.array([]), \n",
    "                 exp_number=0, \n",
    "                 trajectoryLength=50):\n",
    "        self.logger = Logger(exp_name = exp_name, output_dir=dataDirectory + \"/%i/\"%exp_number + exp_name)\n",
    "        self.ResetToInitialState(trainingData, trainingDataLabel)\n",
    "        self.logger.save_config(locals())\n",
    "        self.trajectoryIndex = 0\n",
    "        self.globalTrajectoryIndex = 0\n",
    "        self.trajectoryLength = trajectoryLength\n",
    "        self.initialMSE = self.parameterEstimator.MSE()\n",
    "        \n",
    "        \n",
    "    def calculateControl(self, stateVector):\n",
    "        raise Exception(\"You need to implement this class.\")\n",
    "        \n",
    "    def updateData(self, state, control, nextStateVector):\n",
    "        self.parameterEstimator.addDataRow(np.vstack([state, control]), nextStateVector)\n",
    "        \n",
    "    def addReward(self, reward):\n",
    "        self.logger.log_tabular(\"Reward\", reward)\n",
    "        \n",
    "    def dumpLogger(self):\n",
    "        self.globalTrajectoryIndex += self.trajectoryIndex\n",
    "        self.logger.log_tabular(\"Trajectory index\", self.trajectoryIndex)\n",
    "        self.logger.log_tabular(\"Trajectory number\", self.trajectoryNumber)\n",
    "        self.logger.log_tabular(\"Global trajectory index\", self.globalTrajectoryIndex)\n",
    "        self.logger.log_tabular(\"MSE\", self.parameterEstimator.MSE())\n",
    "        if (useEstimation):\n",
    "            self.logger.log_tabular(\"Estimation\", np.linalg.norm(self.parameterEstimator.parameterEstimation() - np.hstack([A, B]).T, 2))\n",
    "        self.logger.log_tabular(\"Cost\", self.cost)\n",
    "        self.logger.log_tabular(\"Decrease in MSE per Cost\", (self.initialMSE-self.parameterEstimator.MSE())/self.cost)\n",
    "        self.logger.dump_tabular()\n",
    "            \n",
    "    # Expects the vectors to be in column form, i.e. with a shape (x, 1)\n",
    "    def updateCost(self, state, control, nextStateVector):\n",
    "        self.cost += np.array((state.T @ Q @ state + control.T @ R @ control)).reshape((1,1))[0, 0]\n",
    "        \n",
    "    def updateCostAtEndState(self, endState):\n",
    "        self.cost += np.array(endState.T @ Qfinished @ endState).reshape(1,1)[0,0]\n",
    "        \n",
    "    def ResetToInitialState(self, trainingData, trainingDataLabel):\n",
    "        self.parameterEstimator = LSE(trainingData, trainingDataLabel)\n",
    "        self.cost = 0\n",
    "        self.trajectoryNumber = None\n",
    "        self.ResetTrajectory()\n",
    "        \n",
    "    def getInformationValue(self, state, control):\n",
    "        v = np.vstack([state, control])\n",
    "        return v.T @ self.parameterEstimator.dispersion @ self.parameterEstimator.dispersion @ v / (1 + v.T @ self.parameterEstimator.dispersion @ v)\n",
    "        \n",
    "    def ResetTrajectory(self):\n",
    "        if (self.trajectoryNumber is None):\n",
    "            self.trajectoryNumber = 0\n",
    "        else:\n",
    "            self.trajectoryNumber += 1\n",
    "        self.trajectoryIndex = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The random controller samples from a centered normal distribution with covariance structure diag(variance)\n",
    "class RandomControl(ControlMethod):\n",
    "    def __init__(self, trainingData=np.array([]), trainingDataLabel=np.array([]), exp_number=0, trajectoryLength=50, variance=1):\n",
    "        super(RandomControl, self).__init__(exp_name=\"Random\", \n",
    "                                            trainingData=trainingData, \n",
    "                                            trainingDataLabel=trainingDataLabel, \n",
    "                                            exp_number=exp_number, \n",
    "                                            trajectoryLength=trajectoryLength)\n",
    "        self.variance = variance\n",
    "        \n",
    "    def calculateControl(self, stateVector):\n",
    "        self.trajectoryIndex += 1\n",
    "        controlDimension = self.parameterEstimator.dispersion.shape[0] - stateVector.shape[0]\n",
    "        # Freeze seed so the environment will behave the same way\n",
    "        seed = np.random.get_state()\n",
    "        np.random.seed()\n",
    "        result = np.random.normal([0]*controlDimension, [self.variance]*controlDimension).reshape((controlDimension, 1))\n",
    "        np.random.set_state(seed)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The LQR Control is the base class for all dynamic programming based solutions. This class is the certainty equivalence\n",
    "### controller, which assumes the system matrices are correctly estimated. As there is no probing signal injected to the system,\n",
    "### this is called a passive learner. The controller is approximated by the infinite-horizon, discrete time LQG controller.\n",
    "class LQRControl(ControlMethod):\n",
    "    def __init__(self, trainingData=np.array([]), trainingDataLabel=np.array([]), trajectoryLength=50, exp_name=\"Passive\", exp_number=0):\n",
    "        self.trajectoryLength = trajectoryLength\n",
    "        super(LQRControl, self).__init__(exp_name=exp_name,\n",
    "                                         trainingData=trainingData, \n",
    "                                         trainingDataLabel=trainingDataLabel, \n",
    "                                         exp_number=exp_number,\n",
    "                                         trajectoryLength=trajectoryLength)\n",
    "    \n",
    "    def calculateControl(self, stateVector):\n",
    "        self.trajectoryIndex += 1\n",
    "        if (self.K_hat is None):\n",
    "            self.K_hat = np.array((dare(self.A_hat, self.B_hat, Q, R))[2])\n",
    "        return -self.K_hat @ stateVector\n",
    "\n",
    "    def ResetTrajectory(self):\n",
    "        super(LQRControl, self).ResetTrajectory()\n",
    "        # Get current estimates\n",
    "        matrixEst = self.parameterEstimator.parameterEstimation()\n",
    "        self.A_hat = matrixEst[0:matrixEst.shape[1]].T\n",
    "        self.B_hat = matrixEst[matrixEst.shape[1]:].T\n",
    "        self.K_hat = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The method derived in the thesis, which adds a estimator mean squared error term to the loss function. \n",
    "### Rho is an exploration factor, which is constant here.\n",
    "class DispersionControlWithConstantFactor(LQRControl):\n",
    "    def __init__(self, trainingData=np.array([]), \n",
    "                 trainingDataLabel=np.array([]), \n",
    "                 trajectoryLength=50, \n",
    "                 exp_number=0,\n",
    "                 rho=0,\n",
    "                exp_name=\"DC 0.5\"):\n",
    "        self.rho = rho\n",
    "        super(DispersionControlWithConstantFactor, self).__init__(exp_name=exp_name, \n",
    "                                                trainingData=trainingData,\n",
    "                                                trainingDataLabel=trainingDataLabel, \n",
    "                                                trajectoryLength=trajectoryLength,\n",
    "                                                exp_number=exp_number)\n",
    "        \n",
    "       \n",
    "    def calculateControl(self, stateVector):\n",
    "        self.trajectoryIndex += 1\n",
    "        if (self.K_hat3 is None):\n",
    "            sigma = self.parameterEstimator.dispersion\n",
    "            nx = self.A_hat.shape[0]\n",
    "            nu = self.B_hat.shape[1]\n",
    "            Q_new =  (1-self.rho)*Q - self.rho*(sigma[0:nx,0:nx]@sigma[0:nx,0:nx] + sigma[0:nx,nx:] @ sigma[0:nx,nx:].T)\n",
    "            R_new =  (1-self.rho)*R - self.rho*(sigma[0:nx, nx:].T @ sigma[0:nx, nx:] + sigma[nx:, nx:] @ sigma[nx:, nx:])\n",
    "            # To avoid precision errors, copy upper half to lower half\n",
    "            for i in range(nx):\n",
    "                Q_new[i:nx, i] = Q_new[i, i:nx]\n",
    "            for i in range(nu):\n",
    "                R_new[i:nu,i] = R_new[i,i:nu]\n",
    "            P, L, K = dare(self.A_hat, \n",
    "                     self.B_hat, \n",
    "                     Q_new, \n",
    "                     R_new,  \n",
    "                     S=- self.rho*(sigma[0:nx, 0:nx] @ sigma[0:nx, nx:] + sigma[0:nx, nx:] @ sigma[nx:, nx:]),\n",
    "                     E=np.identity(nx))\n",
    "            self.K_hat3 = np.array(K)\n",
    "        return -self.K_hat3 @ stateVector\n",
    "    \n",
    "    def ResetTrajectory(self):\n",
    "        super(DispersionControlWithConstantFactor, self).ResetTrajectory()\n",
    "        self.K_hat3 = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DispersionControlWithVaryingFactor(DispersionControlWithConstantFactor):\n",
    "    def __init__(self, trainingData=np.array([]), \n",
    "                 trainingDataLabel=np.array([]), \n",
    "                 trajectoryLength=50, \n",
    "                 exp_number=0,\n",
    "                 rho=\"max\"):\n",
    "        self.rhoUpdate=rho.lower()\n",
    "        self.trajectoryNumber = None\n",
    "        self.parameterEstimator = LSE(trainingData, trainingDataLabel)\n",
    "        self.updateRho()\n",
    "        super(DispersionControlWithVaryingFactor, self).__init__(exp_name=\"DC \" + str(rho), \n",
    "                                                trainingData=trainingData,\n",
    "                                                trainingDataLabel=trainingDataLabel, \n",
    "                                                trajectoryLength=trajectoryLength,\n",
    "                                                exp_number=exp_number)\n",
    "    \n",
    "    def updateRho(self):\n",
    "        self.rho = 1 / (1 + self.parameterEstimator.MSE()**2)\n",
    "        if (self.rhoUpdate==\"1/n\"  and not (self.trajectoryNumber is None)):\n",
    "            self.rho = min(self.rho, 1/(self.trajectoryNumber+1))\n",
    "        \n",
    "    # Overwrite to add an update to rho, as well as logging it.\n",
    "    def updateData(self, state, control, nextStateVector):\n",
    "        self.parameterEstimator.addDataRow(np.vstack([state, control]), nextStateVector)\n",
    "        self.updateRho()\n",
    "        self.logger.log_tabular(\"Rho\", self.rho)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### In case the system matrices are known, this calculates th optimal controller. \n",
    "class OptimalLQRControl(LQRControl):\n",
    "    def __init__(self, trainingData=np.array([]), trainingDataLabel=np.array([]), trajectoryLength=50, exp_name=\"Optimal\", exp_number=0):\n",
    "        super(OptimalLQRControl, self).__init__(exp_name=exp_name,\n",
    "                                         trainingData=trainingData, \n",
    "                                         trainingDataLabel=trainingDataLabel, \n",
    "                                         exp_number=exp_number,\n",
    "                                         trajectoryLength=trajectoryLength)\n",
    "        self.K = np.array((dare(A, B, Q, R))[2])\n",
    "    \n",
    "    def calculateControl(self, stateVector):\n",
    "        self.trajectoryIndex += 1\n",
    "        return -self.K @ stateVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
    "class PPOControlAllData(ControlMethod):\n",
    "    def __init__(self,\n",
    "                 observation_space,\n",
    "                 action_space,\n",
    "                 exp_name=\"PPO\", \n",
    "                 trainingData=np.array([]), \n",
    "                 trainingDataLabel=np.array([]), \n",
    "                 exp_number=0, \n",
    "                 trajectoryLength=50):\n",
    "        super(PPOControlAllData, self).__init__(exp_name=exp_name,\n",
    "                                     trainingData=trainingData, \n",
    "                                     trainingDataLabel=trainingDataLabel, \n",
    "                                     exp_number=exp_number,\n",
    "                                     trajectoryLength=trajectoryLength)\n",
    "        actor_critic=core.MLPActorCritic\n",
    "        self.steps_per_epoch=trajectoryLength\n",
    "        epochs=50\n",
    "        self.gamma=0.99\n",
    "        self.clip_ratio=0.2\n",
    "        pi_lr=3e-4\n",
    "        vf_lr=1e-3\n",
    "        self.train_pi_iters=80\n",
    "        self.train_v_iters=80\n",
    "        self.lam=0.97\n",
    "        max_ep_len=1000\n",
    "        self.target_kl=0.01\n",
    "        logger_kwargs=dict()\n",
    "        save_freq=10\n",
    "        self.nx = trainingDataLabel.shape[1]\n",
    "        self.nu = trainingData.shape[1] - trainingDataLabel.shape[1]\n",
    "        obs_dim = (self.nx,)\n",
    "        act_dim = (self.nu,)\n",
    "\n",
    "        self.ac = actor_critic(observation_space, action_space)\n",
    "        self.buf = PPOBuffer(obs_dim, act_dim, trajectoryLength, self.gamma, self.lam)\n",
    "\n",
    "            # Set up optimizers for policy and value function\n",
    "        self.pi_optimizer = Adam(self.ac.pi.parameters(), lr=pi_lr)\n",
    "        self.vf_optimizer = Adam(self.ac.v.parameters(), lr=vf_lr)\n",
    "            \n",
    "\n",
    "    # When the monitor is active, the episode needs to be played out for the video to record. \n",
    "    # This will not count towards the statistics.\n",
    "    def epoch(self, env):\n",
    "        o = env.reset()\n",
    "        cum_r = 0\n",
    "        restarts = 0\n",
    "        for t in range(self.steps_per_epoch):\n",
    "            a, v, logp = self.ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            cum_r += r\n",
    "            if (restarts == 0):\n",
    "                self.updateCost(o.reshape((self.nx,1)), a.reshape((self.nu,1)), next_o.reshape((self.nx, 1)))\n",
    "\n",
    "            # save and log\n",
    "            self.buf.store(o, a, r, v, logp)\n",
    "            \n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "\n",
    "            terminal = d\n",
    "            epoch_ended = t==self.steps_per_epoch-1\n",
    "\n",
    "            if epoch_ended or terminal:\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if not terminal:\n",
    "                    _, v, _ = self.ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                else:\n",
    "                    v = 0\n",
    "                # Reward is sampled only for the first trajectory for logging purposes\n",
    "                if (restarts == 0):\n",
    "                    self.updateCostAtEndState(o.reshape((self.nx, 1)))\n",
    "                    self.trajectoryIndex = t+1\n",
    "                    self.addReward(cum_r)\n",
    "                restarts += 1\n",
    "                self.buf.finish_path(v)\n",
    "                o = env.reset()\n",
    "        self.update()\n",
    "        # All steps are played out, so set the index accordingly.\n",
    "        #self.trajectoryIndex = self.steps_per_epoch\n",
    "        self.dumpLogger()\n",
    "            \n",
    "        # Set up function for computing PPO policy loss\n",
    "    def compute_loss_pi(self, data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = self.ac.pi(obs, act)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clip_adv = torch.clamp(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * adv\n",
    "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        clipped = ratio.gt(1+self.clip_ratio) | ratio.lt(1-self.clip_ratio)\n",
    "        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)\n",
    "\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(self, data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((self.ac.v(obs) - ret)**2).mean()\n",
    "    \n",
    "    def update(self):\n",
    "        data = self.buf.get()\n",
    "\n",
    "        pi_l_old, pi_info_old = self.compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = self.compute_loss_v(data).item()\n",
    "\n",
    "        # Train policy with multiple steps of gradient descent\n",
    "        for i in range(self.train_pi_iters):\n",
    "            self.pi_optimizer.zero_grad()\n",
    "            loss_pi, pi_info = self.compute_loss_pi(data)\n",
    "            kl = mpi_avg(pi_info['kl'])\n",
    "            if kl > 1.5 * self.target_kl:\n",
    "                break\n",
    "            loss_pi.backward()\n",
    "            mpi_avg_grads(self.ac.pi)    # average grads across MPI processes\n",
    "            self.pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(self.train_v_iters):\n",
    "            self.vf_optimizer.zero_grad()\n",
    "            loss_v = self.compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            mpi_avg_grads(self.ac.v)    # average grads across MPI processes\n",
    "            self.vf_optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the base method for every experiment. The env variable is an implementation of an OpenAI environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomControl(size):\n",
    "    return np.random.normal([0]*size, [1]*size)\n",
    "\n",
    "def generateInitialDataForEnv(env, stateDimension, controlDimension, trajectoryLength=50, trajectoryAmount=100):\n",
    "    Y = np.zeros((trajectoryAmount,stateDimension))\n",
    "    X = np.zeros((trajectoryAmount,stateDimension+controlDimension))\n",
    "    initialState = np.zeros((stateDimension, 1))\n",
    "    for rowIndex in range(trajectoryAmount):\n",
    "        newState = env.reset()[0:stateDimension].reshape((stateDimension,1))\n",
    "        for i in range(trajectoryLength):\n",
    "            state = newState\n",
    "            randomAction = randomControl(controlDimension)\n",
    "            newState, reward, done, _ = env.step(randomAction)\n",
    "            newState = newState[0:stateDimension].reshape((stateDimension, 1))\n",
    "            if (done):\n",
    "                break\n",
    "        data = np.vstack([state, randomAction.reshape((controlDimension, 1))]).flatten()\n",
    "        X[rowIndex, :] = data\n",
    "        Y[rowIndex, :] = newState.flatten()\n",
    "    return (X, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(runIndex, \n",
    "               env, \n",
    "               stateDimension, \n",
    "               controlDimension, \n",
    "               trajectoryLength=50, \n",
    "               trajectoryAmount=100, \n",
    "               initialDataSize=100,\n",
    "              videoDirectory=\"\",\n",
    "              referencePoint=np.array([])):\n",
    "    baseEnv = env\n",
    "    X, Y = generateInitialDataForEnv(env, stateDimension, controlDimension, trajectoryLength=trajectoryLength, trajectoryAmount=initialDataSize)\n",
    "    controlMethods = [\n",
    "                      PPOControlAllData(env.observation_space, env.action_space, trainingData=X, trainingDataLabel=Y, trajectoryLength=trajectoryLength, exp_number=runIndex),\n",
    "                      LQRControl(X, Y, trajectoryLength=trajectoryLength, exp_number=runIndex),\n",
    "                      RandomControl(X, Y, trajectoryLength=trajectoryLength, exp_number=runIndex, variance=1), \n",
    "                      #DispersionControlWithConstantFactor(X, Y, trajectoryLength=trajectoryLength, exp_number=runIndex, rho=0.5),\n",
    "                     DispersionControlWithVaryingFactor(X, Y, trajectoryLength=trajectoryLength, exp_number=runIndex, rho=\"max\"),\n",
    "                     DispersionControlWithVaryingFactor(X, Y, trajectoryLength=trajectoryLength, exp_number=runIndex, rho=\"1/n\"),\n",
    "    ]\n",
    "    \n",
    "    if (useEstimation):\n",
    "        controlMethods.append( OptimalLQRControl(X, Y, trajectoryLength=trajectoryLength, exp_number=runIndex))\n",
    "    i = 0\n",
    "    seeds = []\n",
    "    for controlMethod in controlMethods:\n",
    "        for trajectory in range(trajectoryAmount):\n",
    "            # Calculate a seed which is used by every control method on the same trajectory\n",
    "            if (len(seeds) <= trajectory):\n",
    "                np.random.seed()\n",
    "                seed = np.random.get_state()\n",
    "                seeds += [seed]\n",
    "            np.random.set_state(seeds[trajectory])\n",
    "            if (len(videoDirectory) > 0 and runIndex == 0 and (trajectory % 200 == 0) and (controlMethod.__class__.__name__ != 'PPOControlAllData')):\n",
    "                env = gym.wrappers.Monitor(\n",
    "                   env, './videos/' + videoDirectory + '/' +str(trajectoryLength) + \".\" + str(trajectoryAmount)  + \".\" + str(runIndex) + \"/\" + str(i) + '_' + type(controlMethod).__name__ + '/' + str(trajectory), force=True)\n",
    "            if (controlMethod.__class__.__name__ == 'PPOControlAllData'):\n",
    "                controlMethod.epoch(env)\n",
    "            else:\n",
    "                newState = env.reset()[0:stateDimension].reshape((stateDimension, 1))\n",
    "                cumulativeReward = 0\n",
    "                for step in range(trajectoryLength):\n",
    "                    state = newState\n",
    "                    if (referencePoint.shape == state.shape):\n",
    "                        state = state - referencePoint\n",
    "                    try:\n",
    "                        control = controlMethod.calculateControl(state).reshape((controlDimension, 1))\n",
    "                    except Exception as e:\n",
    "                        # Sometimes no finite solution can be found, so we skip this experiment run.\n",
    "                        print(\"Error during run\", runIndex, e, controlMethod)\n",
    "                        return\n",
    "                    newState, reward, done, _ = env.step(control.reshape((controlDimension,)))\n",
    "                    cumulativeReward += reward\n",
    "                    newState = newState[0:stateDimension].reshape((stateDimension, 1))\n",
    "                    controlMethod.updateCost(state, control, newState-referencePoint if newState.shape == referencePoint.shape else newState)\n",
    "                    if (done):\n",
    "                        break\n",
    "                # Only the last data point is used for updating the estimator for ensuring iid data. This can be improved significantly\n",
    "                # but this approach is more simple theory-wise.\n",
    "                controlMethod.updateCostAtEndState(newState)\n",
    "                controlMethod.updateData(state, control, newState)\n",
    "                controlMethod.addReward(cumulativeReward)\n",
    "                controlMethod.dumpLogger()\n",
    "            controlMethod.ResetTrajectory()\n",
    "            env = baseEnv\n",
    "        controlMethod.logger.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Center experiment\n",
    "This section contains the code for the data center experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System matrices Ax + Bu, cost matrices x^T Q x + u^T R u\n",
    "state_dimension = 3\n",
    "A = np.diag([1.1]*state_dimension) + \\\n",
    "np.diag([0.1]*(state_dimension-1), -1) + \\\n",
    "np.diag([0.1]*(state_dimension-1), 1)\n",
    "B = np.identity(state_dimension)\n",
    "Q = np.identity(state_dimension)\n",
    "Qfinished = Q * 1000\n",
    "R = 100*np.identity(state_dimension)\n",
    "useEstimation=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Accepts state and control column vectors with combined dimension ((state_dimension, 1))\n",
    "### Returns state of the data center after applying the state matrices and no random disturbance\n",
    "def dataCenterStepDeterministic(stateVector, controlVector):\n",
    "    # Sanity checks\n",
    "    if (stateVector.ndim == 2 and stateVector.shape[1] != 1 or controlVector.ndim == 2 and controlVector.shape[1] != 1):\n",
    "        raise Exception(\"During the data center step, a column vector was expected. Got a row vector instead.\")\n",
    "    if (stateVector.shape[0] != A.shape[0]):\n",
    "        raise Exception (\"Expected the state vector to have shape {}, got {} instead.\".format(A.shape[0], stateVector.shape[0]))\n",
    "    if (controlVector.shape[0] != B.shape[0]):\n",
    "        raise Exception (\"Expected the control vector to have shape {}, got {} instead.\".format(B.shape[0], controlVector.shape[0]))\n",
    "    return A @ stateVector.reshape((state_dimension, 1)) + B @ controlVector.reshape((state_dimension, 1))\n",
    "\n",
    "def dataCenterStepProbabilistic(stateVector, controlVector, std=0.1):\n",
    "    return dataCenterStepDeterministic(stateVector, controlVector) + np.random.normal([0]*state_dimension, [std]*state_dimension).reshape((state_dimension,1))\n",
    "\n",
    "### Wrapper class to \"implement\" OpenAI interface\n",
    "class DataCenterEnvironment():\n",
    "    def __init__(self, dimension, variance):\n",
    "        self.dim = dimension\n",
    "        self.var = variance\n",
    "        self.std = sqrt(variance)\n",
    "        self.currentState = np.zeros((dimension,))\n",
    "        high = np.array([160]*dimension)\n",
    "        self.observation_space = spaces.Box(-high, high)\n",
    "        self.action_space = spaces.Box(-high, high)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.currentState = dataCenterStepProbabilistic(np.zeros((self.dim,1)), np.zeros((self.dim,1)), std=self.std).reshape((self.dim,))\n",
    "        return self.currentState\n",
    "    \n",
    "    # OpenAI returns flattened states\n",
    "    def step(self, control):\n",
    "        cost = self.currentState @ Q @ self.currentState + control @ R @ control\n",
    "        self.currentState = dataCenterStepProbabilistic(self.currentState, control, std=self.std).reshape((self.dim,))\n",
    "        done = (self.currentState >= 80).any() or  (self.currentState <= -80).any() \n",
    "        if done:\n",
    "            # Terminal cost\n",
    "            cost += self.currentState @ Qfinished @ self.currentState\n",
    "        return (self.currentState, -cost, done, False)\n",
    "\n",
    "### Wrapper to always generate a new environment for an experiment\n",
    "def experimentDataCenter(runIndex, stateDimension, sigma, trajectoryLength=50, trajectoryAmount=100):\n",
    "    env = DataCenterEnvironment(stateDimension, sigma)\n",
    "    experiment(runIndex, env, stateDimension, stateDimension, trajectoryLength=trajectoryLength, trajectoryAmount=trajectoryAmount, initialDataSize=18 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some initial data is needed, we generate it using a random control with standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amountOfRuns=16\n",
    "for trajectoryLength, trajectoryAmount, sigma in [(200, 10000, 0.5), (200, 10000, 1), (200, 10000, 2)]:\n",
    "    dataDirectory=\"./data/DataCenter/n=\" + str(state_dimension) + \"/sigma=\" + str(sigma) + \"/amount=\" + \\\n",
    "str(trajectoryLength) + \".\" + str(trajectoryAmount)  + \".\" + str(amountOfRuns) \n",
    "    try:\n",
    "        Parallel(n_jobs=8, verbose=40)(delayed(experimentDataCenter)(i, state_dimension, sigma, trajectoryLength=trajectoryLength, trajectoryAmount=trajectoryAmount) for i in range(amountOfRuns))\n",
    "        generatePlots(dataDirectory, trajectoryAmount=trajectoryAmount, useEstimation=True, runningMeanSize=100)\n",
    "    except Exception as e:\n",
    "        print(\"Exception during \" + dataDirectory)\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lunar import LunarLanderContinuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useEstimation = False\n",
    "Q = np.identity(6)\n",
    "# punish angular velocity\n",
    "Q[5,5] = 10\n",
    "Q = Q*10\n",
    "Qfinished=Q*10\n",
    "\n",
    "R = np.identity(2)\n",
    "R[0,0] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentLunarLander(runIndex, trajectoryLength=50, trajectoryAmount=100):\n",
    "    env = LunarLanderContinuous()\n",
    "    experiment(runIndex, env, 6, 2,  trajectoryLength=trajectoryLength, trajectoryAmount=trajectoryAmount, initialDataSize=100, videoDirectory=\"\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amountOfRuns=16\n",
    "for trajectoryLength, trajectoryAmount in [(300, 5001)]: \n",
    "    dataDirectory=\"./data/LunarLanderFinal/amount=\" + \\\n",
    "str(trajectoryLength) + \".\" + str(trajectoryAmount)  + \".\" + str(amountOfRuns)\n",
    "    try:\n",
    "        print(dataDirectory)\n",
    "        Parallel(n_jobs=8, verbose=40)(delayed(experimentLunarLander)\n",
    "                           (i, trajectoryLength=trajectoryLength, trajectoryAmount=trajectoryAmount) for i in range(amountOfRuns))\n",
    "        generatePlots(dataDirectory, trajectoryAmount=trajectoryAmount, runningMeanSize=250)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Exception during \" + dataDirectory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusted Cart Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartpole import CartPoleContinuousEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useEstimation = True\n",
    "fps = 50\n",
    "Q = np.identity(4)\n",
    "Q[1,1] = 10\n",
    "Q[3,3] = 100\n",
    "Q *= 10\n",
    "Qfinished = 10*Q\n",
    "R = np.identity(1)\n",
    "A = np.zeros((4,4))\n",
    "A[0,1] = 1\n",
    "A[1,2] = -0.7171\n",
    "A[2,3] = 1\n",
    "A[3,2] = 15.7756\n",
    "A = A/fps + np.identity(4)\n",
    "B = np.zeros((4,1))\n",
    "B[1] = 0.9756\n",
    "B[3] = -1.4634\n",
    "B = B/fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentCartPole(runIndex, trajectoryLength=200, trajectoryAmount=100, std=0.01):\n",
    "    env = CartPoleContinuousEnv(std)\n",
    "    experiment(runIndex, env, 4, 1, trajectoryLength=trajectoryLength, trajectoryAmount=trajectoryAmount, initialDataSize=40,  videoDirectory=\"CartPolePerturbed\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amountOfRuns=16\n",
    "for trajectoryLength, trajectoryAmount, std in [(200, 500, 0.01),(200, 500, 0.02),(200, 500, 0.005)]:\n",
    "    dataDirectory=\"./data/CartPolePerturbedFinal/std=\" + str(std) + \"amount=\" + \\\n",
    "str(trajectoryLength) + \".\" + str(trajectoryAmount)  + \".\" + str(amountOfRuns)\n",
    "    try:\n",
    "        print(dataDirectory)\n",
    "        Parallel(n_jobs=8, verbose=40)(delayed(experimentCartPole)\n",
    "                          (i, trajectoryLength=trajectoryLength, trajectoryAmount=trajectoryAmount) for i in range(amountOfRuns))\n",
    "        generatePlots(dataDirectory, trajectoryAmount=trajectoryAmount, runningMeanSize=50, useEstimation=True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"Exception during \" + dataDirectory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
